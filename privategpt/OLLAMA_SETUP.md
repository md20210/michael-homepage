# ü¶ô Ollama Setup - Lokales LLM (DeepSeek-R1)

## ‚úÖ Was ist Ollama?

Ollama ist ein Tool, um **gro√üe Sprachmodelle (LLMs) lokal** auf deinem PC zu betreiben - **komplett kostenlos**, ohne API-Kosten!

**Vorteile:**
- ‚úÖ **$0 Kosten** - Kein API-Key n√∂tig
- ‚úÖ **100% Privat** - Alles bleibt auf deinem PC
- ‚úÖ **Schnell** - Bei guter Hardware sehr performant
- ‚úÖ **Offline** - Funktioniert ohne Internet

---

## üì¶ Installation

### **Linux/WSL (Ubuntu):**

```bash
# Ollama installieren
curl -fsSL https://ollama.com/install.sh | sh

# Pr√ºfen ob installiert
ollama --version
```

### **Windows:**

1. Download: https://ollama.com/download/windows
2. Installer ausf√ºhren
3. Ollama startet automatisch im Hintergrund

### **macOS:**

```bash
# Mit Homebrew
brew install ollama

# Oder Download: https://ollama.com/download/mac
```

---

## üöÄ DeepSeek-R1 Modell herunterladen

Ollama unterst√ºtzt verschiedene Modellgr√∂√üen. W√§hle je nach Hardware:

### **Option 1: Kleines Modell (1.5B) - F√ºr Tests/schwache Hardware**
```bash
ollama pull deepseek-r1:1.5b
```
- **Gr√∂√üe:** ~1 GB
- **RAM:** 4 GB
- **Geschwindigkeit:** Sehr schnell
- **Qualit√§t:** Gut f√ºr einfache Fragen

### **Option 2: Mittleres Modell (7B) - Empfohlen**
```bash
ollama pull deepseek-r1:7b
```
- **Gr√∂√üe:** ~4.5 GB
- **RAM:** 8 GB
- **Geschwindigkeit:** Schnell
- **Qualit√§t:** Sehr gut

### **Option 3: Gro√ües Modell (14B) - Beste Qualit√§t**
```bash
ollama pull deepseek-r1:14b
```
- **Gr√∂√üe:** ~9 GB
- **RAM:** 16 GB
- **Geschwindigkeit:** Mittel
- **Qualit√§t:** Exzellent

### **Option 4: Sehr gro√ües Modell (70B) - Nur f√ºr Power-Hardware**
```bash
ollama pull deepseek-r1:70b
```
- **Gr√∂√üe:** ~40 GB
- **RAM:** 32+ GB oder GPU
- **Qualit√§t:** Beste Ergebnisse

---

## ‚öôÔ∏è Ollama starten

### **Linux/macOS/WSL:**
```bash
# Ollama Server starten
ollama serve
```

**L√§uft auf:** http://localhost:11434

### **Windows:**
Ollama l√§uft automatisch im Hintergrund nach Installation.

**Pr√ºfen:**
```powershell
# PowerShell
curl http://localhost:11434
```

Ergebnis sollte sein: `Ollama is running`

---

## üß™ Testen

```bash
# Einfacher Test
ollama run deepseek-r1:1.5b "Hallo, wer bist du?"

# Mit Dokument-Kontext
ollama run deepseek-r1:1.5b "Erkl√§re mir maschinelles Lernen in 3 S√§tzen"
```

---

## üîß PrivateGPT Backend konfigurieren

√ñffne `backend/.env` und setze:

```env
# Ollama Config
OLLAMA_BASE_URL=http://localhost:11434

# Welches Modell?
# F√ºr Tests: deepseek-r1:1.5b
# Empfohlen: deepseek-r1:7b
# Beste Qualit√§t: deepseek-r1:14b
```

In `backend/config.py` kannst du das Modell √§ndern:
```python
llm_model: str = "deepseek-r1:7b"  # √Ñndere hier!
```

---

## üí° Performance-Tipps

### **GPU-Beschleunigung (NVIDIA):**
Ollama nutzt automatisch deine GPU, wenn verf√ºgbar!

**Pr√ºfen:**
```bash
nvidia-smi  # Zeigt GPU-Nutzung
```

### **RAM zu knapp?**
Verwende ein kleineres Modell:
```bash
ollama pull deepseek-r1:1.5b
```

### **Zu langsam?**
- Schlie√üe andere Programme
- Nutze GPU (falls vorhanden)
- Wechsle zu kleinerem Modell

---

## üìä Modell-Vergleich

| Modell | Gr√∂√üe | RAM | Qualit√§t | Geschwindigkeit | Empfehlung |
|--------|-------|-----|----------|----------------|------------|
| 1.5B | 1 GB | 4 GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Tests |
| 7B | 4.5 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | ‚úÖ **Empfohlen** |
| 14B | 9 GB | 16 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Power-User |
| 70B | 40 GB | 32 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | Server/GPU |

---

## üõ†Ô∏è Troubleshooting

### **"Ollama not found"**
```bash
# Linux/WSL: Installiere neu
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Starte Ollama Desktop App
```

### **"Connection refused"**
```bash
# Ollama l√§uft nicht - starte:
ollama serve

# Oder Windows: Starte Ollama App
```

### **"Model not found"**
```bash
# Modell herunterladen
ollama pull deepseek-r1:1.5b

# Verf√ºgbare Modelle anzeigen
ollama list
```

### **Zu langsam / Out of Memory**
```bash
# Wechsel zu kleinerem Modell
ollama pull deepseek-r1:1.5b

# In backend/config.py √§ndern:
llm_model: str = "deepseek-r1:1.5b"
```

---

## üéØ N√§chste Schritte

1. ‚úÖ Ollama installiert
2. ‚úÖ Modell heruntergeladen
3. ‚úÖ `ollama serve` l√§uft

**Jetzt:**
```bash
# Backend starten
cd backend
./start-backend.sh

# Frontend starten (neues Terminal)
cd frontend
./start-frontend.sh
```

**√ñffne:** http://localhost:5173

---

## üí∞ Kosten: $0!

Kein API-Key n√∂tig, alles lokal, **komplett kostenlos**! üéâ

**Sp√§ter f√ºr Railway (Cloud):**
- GPU-Instanz: ~$30-200/Monat (je nach Modell)
- Aber f√ºr lokale Entwicklung: **$0**
